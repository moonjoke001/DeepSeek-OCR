# DeepSeek-OCR æ¨¡å¼ä½¿ç”¨æŒ‡å—

## ğŸ“Š 5ç§åˆ†è¾¨ç‡æ¨¡å¼å¯¹æ¯”

| æ¨¡å¼ | base_size | image_size | crop_mode | æ¯é¡µ Tokens | æœ€å¤§é¡µæ•° | ç²¾åº¦ | é€Ÿåº¦ | é€‚ç”¨åœºæ™¯ |
|------|-----------|------------|-----------|-------------|----------|------|------|----------|
| **Tiny** | 512 | 512 | False | ~50 | 160+ | ä½ | æœ€å¿« | ç®€å•æ”¶æ®ã€æ ‡ç­¾ |
| **Small** | 640 | 640 | False | ~75 | 110 | ä¸­ | å¿« | æ™®é€šå‘ç¥¨ã€è¡¨å• |
| **Base** | 1024 | 1024 | False | ~100 | 80 | é«˜ | ä¸­ | æ ‡å‡†æ–‡æ¡£ã€åˆåŒ |
| **Large** | 1280 | 1280 | False | ~150 | 55 | å¾ˆé«˜ | æ…¢ | å¤æ‚å›¾è¡¨ã€æŠ€æœ¯æ–‡æ¡£ |
| **Gundam** | 1024 | 640 | True | ~100-150 | 55-80 | é«˜ | ä¸­ | é•¿æ–‡æ¡£ã€ä¹¦ç± |

## ğŸ”§ ä½¿ç”¨æ–¹æ³•

### æ–¹æ³• 1: Python API ç›´æ¥è°ƒç”¨

```python
from vllm import LLM, SamplingParams
from vllm.model_executor.models.deepseek_ocr import NGramPerReqLogitsProcessor
from PIL import Image

# åˆ›å»ºæ¨¡å‹å®ä¾‹
llm = LLM(
    model="deepseek-ai/DeepSeek-OCR",
    enable_prefix_caching=False,
    mm_processor_cache_gb=0,
    logits_processors=[NGramPerReqLogitsProcessor]
)

# å‡†å¤‡å›¾ç‰‡
image = Image.open("document.jpg").convert("RGB")

# é€‰æ‹©æ¨¡å¼ - é€šè¿‡ prompt ä¸­çš„ç‰¹æ®Šæ ‡è®°
# æ³¨æ„: vLLM å½“å‰ç‰ˆæœ¬ä¸æ”¯æŒç›´æ¥ä¼ é€’ base_size å‚æ•°
# éœ€è¦åœ¨å›¾åƒé¢„å¤„ç†é˜¶æ®µè°ƒæ•´

prompt = "<image>\nFree OCR."

model_input = {
    "prompt": prompt,
    "multi_modal_data": {"image": image}
}

sampling_param = SamplingParams(
    temperature=0.0,
    max_tokens=8192,
    extra_args=dict(
        ngram_size=30,
        window_size=90,
        whitelist_token_ids={128821, 128822},
    ),
    skip_special_tokens=False,
)

# ç”Ÿæˆè¾“å‡º
outputs = llm.generate([model_input], sampling_param)
print(outputs[0].outputs[0].text)
```

### æ–¹æ³• 2: ä½¿ç”¨ Transformers (æ”¯æŒå®Œæ•´æ¨¡å¼æ§åˆ¶)

```python
from transformers import AutoModel, AutoTokenizer
import torch

model_name = 'deepseek-ai/DeepSeek-OCR'
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
model = AutoModel.from_pretrained(
    model_name, 
    _attn_implementation='flash_attention_2',
    trust_remote_code=True,
    use_safetensors=True
)
model = model.eval().cuda().to(torch.bfloat16)

prompt = "<image>\n<|grounding|>Convert the document to markdown."
image_file = 'document.jpg'
output_path = './output'

# ===== é€‰æ‹©æ¨¡å¼ =====

# Tiny æ¨¡å¼ - æœ€å¿«
res = model.infer(
    tokenizer, 
    prompt=prompt, 
    image_file=image_file,
    output_path=output_path,
    base_size=512,
    image_size=512,
    crop_mode=False,
    save_results=True
)

# Small æ¨¡å¼ - å¿«é€Ÿ
res = model.infer(
    tokenizer,
    prompt=prompt,
    image_file=image_file,
    output_path=output_path,
    base_size=640,
    image_size=640,
    crop_mode=False,
    save_results=True
)

# Base æ¨¡å¼ - æ¨è (é»˜è®¤)
res = model.infer(
    tokenizer,
    prompt=prompt,
    image_file=image_file,
    output_path=output_path,
    base_size=1024,
    image_size=1024,
    crop_mode=False,
    save_results=True
)

# Large æ¨¡å¼ - é«˜è´¨é‡
res = model.infer(
    tokenizer,
    prompt=prompt,
    image_file=image_file,
    output_path=output_path,
    base_size=1280,
    image_size=1280,
    crop_mode=False,
    save_results=True
)

# Gundam æ¨¡å¼ - é•¿æ–‡æ¡£ (æ¨è)
res = model.infer(
    tokenizer,
    prompt=prompt,
    image_file=image_file,
    output_path=output_path,
    base_size=1024,
    image_size=640,
    crop_mode=True,  # å¯ç”¨è£å‰ªæ¨¡å¼
    save_results=True,
    test_compress=True  # æ˜¾ç¤ºå‹ç¼©ç»Ÿè®¡
)
```

### æ–¹æ³• 3: é€šè¿‡ HTTP API (å½“å‰ Docker éƒ¨ç½²)

**æ³¨æ„**: å½“å‰çš„ vLLM Docker éƒ¨ç½²**ä¸æ”¯æŒ**ç›´æ¥æŒ‡å®šæ¨¡å¼å‚æ•°ã€‚

vLLM çš„ OpenAI å…¼å®¹ API åªæ¥å—æ ‡å‡†çš„å›¾åƒè¾“å…¥,æ¨¡å¼å‚æ•°éœ€è¦åœ¨**å®¢æˆ·ç«¯é¢„å¤„ç†**æ—¶æŒ‡å®šã€‚

#### è§£å†³æ–¹æ¡ˆ:

1. **å®¢æˆ·ç«¯é¢„å¤„ç†** - åœ¨å‘é€åˆ° API å‰è°ƒæ•´å›¾åƒå¤§å°
2. **ä½¿ç”¨ Transformers** - ç›´æ¥ä½¿ç”¨ transformers åº“è€Œä¸æ˜¯ vLLM
3. **æ‰©å±• Web UI** - æ·»åŠ æ¨¡å¼é€‰æ‹©åŠŸèƒ½

## ğŸ¯ æ¨èé…ç½®

### åœºæ™¯ 1: å‘ç¥¨/æ”¶æ®å¤„ç†
```python
base_size=640, image_size=640, crop_mode=False  # Small æ¨¡å¼
```

### åœºæ™¯ 2: æ ‡å‡†æ–‡æ¡£/åˆåŒ
```python
base_size=1024, image_size=1024, crop_mode=False  # Base æ¨¡å¼
```

### åœºæ™¯ 3: æŠ€æœ¯æ–‡æ¡£/å›¾è¡¨
```python
base_size=1280, image_size=1280, crop_mode=False  # Large æ¨¡å¼
```

### åœºæ™¯ 4: é•¿æ–‡æ¡£/ä¹¦ç±
```python
base_size=1024, image_size=640, crop_mode=True  # Gundam æ¨¡å¼
```

## âš ï¸ é‡è¦æç¤º

1. **vLLM é™åˆ¶**: å½“å‰ vLLM éƒ¨ç½²ä½¿ç”¨é»˜è®¤æ¨¡å¼,æ— æ³•é€šè¿‡ API åŠ¨æ€åˆ‡æ¢
2. **æ€§èƒ½æƒè¡¡**: æ›´é«˜åˆ†è¾¨ç‡ = æ›´å¥½è´¨é‡ä½†æ›´æ…¢é€Ÿåº¦
3. **Token é¢„ç®—**: é«˜åˆ†è¾¨ç‡æ¨¡å¼ä¼šæ¶ˆè€—æ›´å¤š tokens,å‡å°‘å¯å¤„ç†é¡µæ•°
4. **æ¨èé»˜è®¤**: Gundam æ¨¡å¼ (base_size=1024, image_size=640, crop_mode=True)

## ğŸ”„ å¦‚ä½•åœ¨ Docker éƒ¨ç½²ä¸­åˆ‡æ¢æ¨¡å¼

ç›®å‰çš„ Docker éƒ¨ç½²**ä¸æ”¯æŒ**è¿è¡Œæ—¶åˆ‡æ¢æ¨¡å¼ã€‚å¦‚éœ€ä½¿ç”¨ä¸åŒæ¨¡å¼:

### é€‰é¡¹ 1: ä½¿ç”¨ Transformers è€Œé vLLM
éƒ¨ç½²ä¸€ä¸ªåŸºäº transformers çš„æœåŠ¡,æ”¯æŒå®Œæ•´çš„æ¨¡å¼å‚æ•°

### é€‰é¡¹ 2: å®¢æˆ·ç«¯é¢„å¤„ç†
åœ¨è°ƒç”¨ API å‰,æŒ‰ç…§ç›®æ ‡æ¨¡å¼è°ƒæ•´å›¾åƒå°ºå¯¸

### é€‰é¡¹ 3: æ‰©å±• vLLM (é«˜çº§)
ä¿®æ”¹ vLLM æºç ,æ·»åŠ è‡ªå®šä¹‰å‚æ•°æ”¯æŒ

## ğŸ“š å‚è€ƒèµ„æ–™

- [å®˜æ–¹ GitHub](https://github.com/deepseek-ai/DeepSeek-OCR)
- [Hugging Face æ¨¡å‹é¡µ](https://huggingface.co/deepseek-ai/DeepSeek-OCR)
- [vLLM æ–‡æ¡£](https://docs.vllm.ai/projects/recipes/en/latest/DeepSeek/DeepSeek-OCR.html)
